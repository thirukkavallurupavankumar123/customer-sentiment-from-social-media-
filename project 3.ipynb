{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b408f-7d9f-4c5e-b852-20855fa41b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_full_pipeline.py\n",
    "# Single-file implementation: Data collection -> preprocessing -> VADER -> TF-IDF+LogReg -> BERT -> visualization -> save\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP & ML\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Scraping & Wordcloud\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ------------------------\n",
    "# Configuration\n",
    "# ------------------------\n",
    "BRAND = \"Tesla\"                           # change brand here\n",
    "LANG = \"en\"\n",
    "SINCE = \"2025-07-01\"                      # inclusive\n",
    "UNTIL = \"2025-08-01\"                      # exclusive\n",
    "MAX_TWEETS = 1000                         # number of tweets to collect\n",
    "OUTPUT_DIR = \"sentiment_outputs\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# NLTK setup\n",
    "# ------------------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# ------------------------\n",
    "# 1. Data Collection (snscrape)\n",
    "# ------------------------\n",
    "def collect_tweets(query_brand=BRAND, lang=LANG, since=SINCE, until=UNTIL, max_tweets=MAX_TWEETS):\n",
    "    query = f\"{query_brand} lang:{lang} since:{since} until:{until}\"\n",
    "    tweets = []\n",
    "    print(f\"Collecting up to {max_tweets} tweets with query: {query}\")\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i >= max_tweets:\n",
    "            break\n",
    "        tweets.append({\n",
    "            \"date\": tweet.date,\n",
    "            \"tweet\": tweet.content,\n",
    "            \"username\": tweet.user.username\n",
    "        })\n",
    "    df = pd.DataFrame(tweets)\n",
    "    print(f\"Collected {len(df)} tweets.\")\n",
    "    return df\n",
    "\n",
    "# ------------------------\n",
    "# 2. Preprocessing\n",
    "# ------------------------\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove URLs, mentions, hashtags, punctuation (keep spaces), numbers\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)  # keep the word but remove '#'\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)  # keep only letters and spaces\n",
    "    text = text.lower()\n",
    "    # tokenization & remove stopwords\n",
    "    tokens = [tok for tok in text.split() if tok not in STOPWORDS and len(tok) > 1]\n",
    "    # lemmatize\n",
    "    tokens = [LEMMATIZER.lemmatize(tok) for tok in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ------------------------\n",
    "# 3. VADER Sentiment\n",
    "# ------------------------\n",
    "def apply_vader(df, text_col='clean_tweet'):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['vader_compound'] = df[text_col].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "    df['vader_label'] = df['vader_compound'].apply(lambda s: 'Positive' if s > 0.05 else ('Negative' if s < -0.05 else 'Neutral'))\n",
    "    return df\n",
    "\n",
    "# ------------------------\n",
    "# 4. TF-IDF + Logistic Regression (using VADER labels as pseudo-labels)\n",
    "# ------------------------\n",
    "def train_tfidf_logreg(df, text_col='clean_tweet', label_col='vader_label'):\n",
    "    # Prepare data\n",
    "    X = df[text_col].fillna(\"\").values\n",
    "    y = df[label_col].values\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "    X_vect = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    print(\"Training LogisticRegression on TF-IDF features...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"TF-IDF + Logistic Regression results:\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # save model artifacts\n",
    "    import pickle\n",
    "    with open(os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    with open(os.path.join(OUTPUT_DIR, 'logreg_model.pkl'), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Save test results\n",
    "    test_df = pd.DataFrame({\n",
    "        \"text\": X_test.astype(str)[:10] if hasattr(X_test, \"astype\") else [],\n",
    "        \"true\": y_test,\n",
    "        \"pred\": y_pred\n",
    "    })\n",
    "    return model, vectorizer, acc\n",
    "\n",
    "# ------------------------\n",
    "# 5. BERT Sentiment (transformers pipeline)\n",
    "# ------------------------\n",
    "def apply_bert(df, raw_text_col='tweet', batch_size=16):\n",
    "    print(\"Loading BERT sentiment pipeline (this will download the model)...\")\n",
    "    bert = pipeline(\"sentiment-analysis\", truncation=True)\n",
    "    labels = []\n",
    "    scores = []\n",
    "    texts = df[raw_text_col].fillna(\"\").tolist()\n",
    "    print(\"Classifying with BERT (may take time depending on CPU/GPU)...\")\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        out = bert(batch)\n",
    "        for o in out:\n",
    "            labels.append(o['label'])\n",
    "            scores.append(o.get('score', None))\n",
    "    # Normalize labels (some models return 'POSITIVE'/'NEGATIVE')\n",
    "    labels = [l.title() for l in labels]  # 'POSITIVE' -> 'Positive'\n",
    "    df['bert_label'] = labels\n",
    "    df['bert_score'] = scores\n",
    "    return df\n",
    "\n",
    "# ------------------------\n",
    "# 6. Visualization utilities\n",
    "# ------------------------\n",
    "def plot_sentiment_distribution(df, label_col, title, filename):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    order = ['Positive','Neutral','Negative']\n",
    "    sns.countplot(x=label_col, data=df, order=order)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_wordcloud_from_texts(texts, filename, max_words=150):\n",
    "    text = \" \".join(texts)\n",
    "    wc = WordCloud(width=1200, height=600, background_color='white', max_words=max_words).generate(text)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_time_series_sentiment(df, date_col='date', label_col='vader_label', filename='sentiment_timeseries.png'):\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df['date_only'] = df[date_col].dt.date\n",
    "    daily = df.groupby(['date_only', label_col]).size().unstack(fill_value=0)\n",
    "    daily = daily.reindex(columns=['Positive','Neutral','Negative'], fill_value=0)\n",
    "    daily.plot(kind='line', figsize=(10,5), marker='o')\n",
    "    plt.title('Daily sentiment counts')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, filename), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ------------------------\n",
    "# 7. Main pipeline\n",
    "# ------------------------\n",
    "def main():\n",
    "    # 1. collect\n",
    "    df = collect_tweets()\n",
    "    if df.empty:\n",
    "        print(\"No tweets collected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # save raw\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'raw_tweets.csv'), index=False)\n",
    "\n",
    "    # 2. preprocess\n",
    "    print(\"Cleaning tweets...\")\n",
    "    df['clean_tweet'] = df['tweet'].apply(clean_text)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'cleaned_tweets.csv'), index=False)\n",
    "\n",
    "    # basic info\n",
    "    print(\"Sample cleaned tweets:\")\n",
    "    print(df['clean_tweet'].head(5).to_list())\n",
    "\n",
    "    # 3. VADER\n",
    "    df = apply_vader(df)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'with_vader.csv'), index=False)\n",
    "    plot_sentiment_distribution(df, 'vader_label', 'Sentiment Distribution (VADER)', 'vader_distribution.png')\n",
    "\n",
    "    # 4. TF-IDF + Logistic Regression\n",
    "    model, vectorizer, tfidf_acc = train_tfidf_logreg(df)\n",
    "    # store a prediction column using logreg for all data\n",
    "    X_all = vectorizer.transform(df['clean_tweet'].fillna(\"\").tolist())\n",
    "    df['logreg_pred'] = model.predict(X_all)\n",
    "\n",
    "    # 5. BERT classification\n",
    "    df = apply_bert(df)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'with_all_models.csv'), index=False)\n",
    "\n",
    "    # 6. Visualizations\n",
    "    plot_sentiment_distribution(df, 'vader_label', 'VADER Sentiment Distribution', 'vader_distribution.png')\n",
    "    plot_sentiment_distribution(df, 'logreg_pred', 'LogReg (TF-IDF) Sentiment Distribution', 'logreg_distribution.png')\n",
    "    plot_sentiment_distribution(df, 'bert_label', 'BERT Sentiment Distribution', 'bert_distribution.png')\n",
    "\n",
    "    # wordclouds for positive & negative (by VADER)\n",
    "    positive_texts = df[df['vader_label']=='Positive']['clean_tweet'].tolist()\n",
    "    negative_texts = df[df['vader_label']=='Negative']['clean_tweet'].tolist()\n",
    "    if positive_texts:\n",
    "        plot_wordcloud_from_texts(positive_texts, 'wordcloud_positive_vader.png')\n",
    "    if negative_texts:\n",
    "        plot_wordcloud_from_texts(negative_texts, 'wordcloud_negative_vader.png')\n",
    "\n",
    "    # time series (VADER)\n",
    "    plot_time_series_sentiment(df, label_col='vader_label', filename='vader_timeseries.png')\n",
    "\n",
    "    # 7. Simple evaluation: compare VADER vs LogReg vs BERT\n",
    "    # We'll compute agreement metrics (accuracy against VADER for logreg and bert)\n",
    "    # Note: TF-IDF/LogReg trained on VADER pseudo-labels; BERT is independent.\n",
    "    print(\"\\nAgreement/Evaluation summary (reference = VADER pseudo-label):\")\n",
    "    for col in ['logreg_pred', 'bert_label']:\n",
    "        # drop NaNs\n",
    "        valid = df[~df[col].isna()]\n",
    "        agree = (valid[col].values == valid['vader_label'].values).mean()\n",
    "        print(f\"{col} agreement with VADER: {agree:.4f} ({int(agree*100)}%)\")\n",
    "\n",
    "    # Confusion table example (vader vs bert)\n",
    "    cm = pd.crosstab(df['vader_label'], df['bert_label'], rownames=['VADER'], colnames=['BERT'], normalize='index')\n",
    "    cm.to_csv(os.path.join(OUTPUT_DIR, 'confusion_vader_bert.csv'))\n",
    "    print(\"\\nSaved outputs to\", OUTPUT_DIR)\n",
    "    print(\"Top-level files:\")\n",
    "    for fname in os.listdir(OUTPUT_DIR):\n",
    "        print(\" -\", fname)\n",
    "\n",
    "    # Save final dataframe\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'final_results.csv'), index=False)\n",
    "    # Save summary JSON\n",
    "    summary = {\n",
    "        \"brand\": BRAND,\n",
    "        \"since\": SINCE,\n",
    "        \"until\": UNTIL,\n",
    "        \"n_tweets\": len(df),\n",
    "        \"tfidf_logreg_accuracy_est\": tfidf_acc\n",
    "    }\n",
    "    with open(os.path.join(OUTPUT_DIR, 'summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
